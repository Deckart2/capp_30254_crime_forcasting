{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpers to Run ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_mean(df_train, df_to_fill):\n",
    "    '''\n",
    "    A simple function that fills missing values of continuous columns \n",
    "        with the column median\n",
    "    Inputs:\n",
    "        df_train (df): the training df. Function computes means from this value AND fills this \n",
    "        df_to_fill (df): the df whose continuous NAs should be filled \n",
    "    Returns:\n",
    "        df_train (df): the training dataset\n",
    "        df_to_fill: the testing dataset with its data filled by the training data median\n",
    "\n",
    "    '''\n",
    "    df_train_num = df_train.select_dtypes(include=[np.number])\n",
    "    means = df_train_num.mean().to_dict()\n",
    "    \n",
    "    df_train = df_train.fillna(value=means)\n",
    "    df_to_fill = df_to_fill.fillna(value=means)\n",
    "    print(\"Finished filling NAs with mean...\")\n",
    "    return df_train, df_to_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_continuous(df, scaler = None):\n",
    "    '''\n",
    "    A simple function that normalizes the values of of continuous columns \n",
    "        using data from the training set\n",
    "    Inputs:\n",
    "        df (df): either the training or the testing df\n",
    "        scaler: the scaler object. It will be None for training and exist for testing \n",
    "    Returns:\n",
    "        df (df): the standardized df\n",
    "        scaler: the scaler object\n",
    "    '''\n",
    "    if scaler is None: #Training case\n",
    "        scaler = sk.preprocessing.StandardScaler() #Set up scaler\n",
    "        df_num = df.select_dtypes(include=[np.number]) #find numeric columns\n",
    "        df_num_scaled = scaler.fit_transform(df_num) #Normalize them\n",
    "        df_num_cols = list(df_num.columns) \n",
    "        df.loc[:, df_num_cols] = df_num_scaled #Insert columns back into the main df \n",
    "        print(\"Finished normalizing training data\")\n",
    "    else: #Testing case\n",
    "        df_num = df.select_dtypes(include=[np.number]) #find numeric columns\n",
    "        df_num_scaled = scaler.transform(df_num) #Normalize them\n",
    "        df_num_cols = list(df_num.columns) \n",
    "        df.loc[:, df_num_cols] = df_num_scaled #Insert columns back into the main df \n",
    "        print(\"Finished normalizing test data...\")\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, cat_vars): \n",
    "    '''\n",
    "    A function to one-hot encode given categorical variables\n",
    "    Inputs:\n",
    "        df (df): a pandas dataframe\n",
    "        cat_vars (list of strings): a list of the categorical variables to one-hot encode\n",
    "    '''\n",
    "\n",
    "    df = pd.get_dummies(df, columns = cat_vars)\n",
    "    print(\"finished one-hot encoding...\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(train, test):\n",
    "    '''\n",
    "    A function to ensure that training and testing data have identical columns\n",
    "    after one-hot encoding\n",
    "    If a column is in training but not testing, adds a column of 0s to testing\n",
    "    If column is in testing but not training, it is removed\n",
    "    Inputs:\n",
    "        train (df): the training df\n",
    "        test (df): the testing df\n",
    "    Outputs:\n",
    "        train, test (df): the datasets with identical columns\n",
    "    '''\n",
    "    train_cols = list(train.columns)\n",
    "    test_cols = list(test.columns)\n",
    "    \n",
    "    for tr_col in train_cols:\n",
    "        if tr_col not in test_cols:\n",
    "            test[tr_col] = 0\n",
    "    \n",
    "    for test_col in test_cols:\n",
    "        if test_col not in train_cols:\n",
    "            test = test.drop(test_col, axis=1)\n",
    "    print(\"finished standardizing...\")\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_year(df, y, test_year, num_years):\n",
    "    '''\n",
    "    isin syntax from: https://www.kite.com/python/answers/how-to-filter-a-pandas-dataframe-with-a-list-by-%60in%60-or-%60not-in%60-in-python\n",
    "    Inputs:\n",
    "        df (df): the dataframe with both the X and y \n",
    "        y (string): the column name in the df that is the target\n",
    "        test_year(int): the year we seek to predict in 2016-2020\n",
    "        num_years(int): the number of years to be included in the training set\n",
    "\n",
    "    Output:\n",
    "    train_df (df): A training dataframe\n",
    "    test_X (df): Testing df of predictors\n",
    "    test_y (df): Testing target\n",
    "    '''\n",
    "    year_range = np.arange(test_year - num_years, test_year)\n",
    "    train_filter = df.Year.isin(year_range)\n",
    "    train_df = df[train_filter]\n",
    "    train_X = train_df.drop(columns=[y])\n",
    "    train_y = train_df[y]\n",
    "    \n",
    "    test_df = df[df.Year==test_year]\n",
    "    test_X = test_df.drop(columns = [y], inplace=True)\n",
    "    test_y = test_df[y]\n",
    "    print(\"finished splitting by year...\")\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df, y, test_year, num_years, vars_to_onehot):\n",
    "    '''\n",
    "    Helper function that aggregates the above helpers to prepare for imputation in \n",
    "    an ML algorithm. Specifically this:\n",
    "        Splits the training set and testing set based on year \n",
    "            using split_train_test_by_year\n",
    "        One-hot encodes and standardizes the columns using \n",
    "            one_hot_encode and standardize_column\n",
    "        Normalizes all continuous variables using normalize_continuous\n",
    "    Inputs:\n",
    "    df (pandas DataFrame): the dataframe with training and testing data, predictors and target\n",
    "    y (string): the name of the target column\n",
    "    num_years (int): the number of years to be included in the training set\n",
    "    test_year (int): the year we seek to predict 2015_2020\n",
    "    \n",
    "    Outputs:\n",
    "    train_df (df): a standardized training set with one-hot encorded categorical columns\n",
    "    test_df (df): the test dataframe, again standardized as above\n",
    "    test_y (Series): the test target\n",
    "    '''\n",
    "    train_df, train_y, test_df, test_y = split_train_test_by_year(df, y, test_year, num_years)\n",
    "    train_df, test_df = standardize_columns(train_df, test_df)\n",
    "    train_df, test_df = fill_missing_mean(train_df, test_df)\n",
    "    train_df, scaler = normalize_continuous(train_df)\n",
    "    test_df, doesnt_matter = normalize_continuous(test_df, scaler)\n",
    "    train_df = one_hot_encode(train_df, vars_to_onehot)\n",
    "    test_df = one_hot_encode(test_df, vars_to_onehot)\n",
    "    train_df, test_df = standardize_columns(train_df, test_df)\n",
    "    return train_df, train_y, test_df, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../intermediate_data/df_2015_to_present.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_categorical(df, cols_to_convert):\n",
    "    '''\n",
    "    Convert columns to categorical\n",
    "    Inputs:\n",
    "        df (pd.DataFrame): The Pandas df\n",
    "        cols_to_convert (list of strings): The columns to convert    \n",
    "    Output:\n",
    "        df - the updated dataframe\n",
    "    '''\n",
    "    for col in cols_to_convert:\n",
    "        df[col]=df[col].astype(\"category\")\n",
    "\n",
    "    return df\n",
    "\n",
    "data = convert_to_categorical(data, [\"Beat\", \"Year\", \"Month\", \"Watch\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished splitting by year...\n"
     ]
    }
   ],
   "source": [
    "train_df, train_y, test_df, test_y = split_train_test_by_year(data, \"Arrest\", 2017, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished splitting by year...\n",
      "finished standardizing...\n",
      "Finished filling NAs with mean...\n",
      "Finished normalizing training data\n",
      "Finished normalizing test data...\n",
      "finished one-hot encoding...\n",
      "finished one-hot encoding...\n",
      "finished standardizing...\n"
     ]
    }
   ],
   "source": [
    "#Test the code! \n",
    "train_X, train_y, test_X, test_y = prep_data(data, \"Arrest\", 2017, 2, \n",
    "                                       [\"Year\", \"Month\", \"Week\", \"Beat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>count_l_stops</th>\n",
       "      <th>count_bus_stops</th>\n",
       "      <th>count_metra_stops</th>\n",
       "      <th>...</th>\n",
       "      <th>Beat_2521</th>\n",
       "      <th>Beat_2522</th>\n",
       "      <th>Beat_2523</th>\n",
       "      <th>Beat_2524</th>\n",
       "      <th>Beat_2525</th>\n",
       "      <th>Beat_2531</th>\n",
       "      <th>Beat_2532</th>\n",
       "      <th>Beat_2533</th>\n",
       "      <th>Beat_2534</th>\n",
       "      <th>Beat_2535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>534090.000000</td>\n",
       "      <td>534090.000000</td>\n",
       "      <td>534090.000000</td>\n",
       "      <td>534090.000000</td>\n",
       "      <td>534090.000000</td>\n",
       "      <td>534090.000000</td>\n",
       "      <td>534090.00000</td>\n",
       "      <td>534090.000000</td>\n",
       "      <td>534090.000000</td>\n",
       "      <td>534090.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.833968e-15</td>\n",
       "      <td>-2.426935e-14</td>\n",
       "      <td>1.807149e-14</td>\n",
       "      <td>-6.361288e-14</td>\n",
       "      <td>-1.527197e-14</td>\n",
       "      <td>-1.510259e-14</td>\n",
       "      <td>1.499653e-13</td>\n",
       "      <td>-8.963044e-14</td>\n",
       "      <td>-1.005280e-15</td>\n",
       "      <td>-7.794098e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.003073</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.00454</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.005198</td>\n",
       "      <td>0.003763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065722</td>\n",
       "      <td>0.055345</td>\n",
       "      <td>0.053655</td>\n",
       "      <td>0.050508</td>\n",
       "      <td>0.047622</td>\n",
       "      <td>0.058879</td>\n",
       "      <td>0.06723</td>\n",
       "      <td>0.077173</td>\n",
       "      <td>0.071907</td>\n",
       "      <td>0.061231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.796652e+01</td>\n",
       "      <td>-1.653279e+00</td>\n",
       "      <td>-1.993694e+00</td>\n",
       "      <td>-3.779970e-01</td>\n",
       "      <td>-1.358532e-01</td>\n",
       "      <td>-2.747795e+00</td>\n",
       "      <td>-2.873175e+00</td>\n",
       "      <td>-5.721857e-01</td>\n",
       "      <td>-2.123496e+00</td>\n",
       "      <td>-4.499297e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.557260e-01</td>\n",
       "      <td>-8.633741e-01</td>\n",
       "      <td>-6.294804e-01</td>\n",
       "      <td>-3.779970e-01</td>\n",
       "      <td>-1.358532e-01</td>\n",
       "      <td>-7.970072e-01</td>\n",
       "      <td>-7.117863e-01</td>\n",
       "      <td>-5.721857e-01</td>\n",
       "      <td>-7.038846e-01</td>\n",
       "      <td>-4.499297e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.738473e-02</td>\n",
       "      <td>3.937406e-02</td>\n",
       "      <td>1.284163e-01</td>\n",
       "      <td>-3.779970e-01</td>\n",
       "      <td>-1.358532e-01</td>\n",
       "      <td>1.783866e-01</td>\n",
       "      <td>2.624904e-02</td>\n",
       "      <td>-5.721857e-01</td>\n",
       "      <td>-3.063934e-01</td>\n",
       "      <td>-4.499297e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.263588e-01</td>\n",
       "      <td>8.292787e-01</td>\n",
       "      <td>8.863130e-01</td>\n",
       "      <td>-2.190015e-01</td>\n",
       "      <td>-1.358532e-01</td>\n",
       "      <td>8.611624e-01</td>\n",
       "      <td>9.224348e-01</td>\n",
       "      <td>6.704085e-01</td>\n",
       "      <td>5.453734e-01</td>\n",
       "      <td>-4.499297e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.472606e+00</td>\n",
       "      <td>1.732027e+00</td>\n",
       "      <td>1.492630e+00</td>\n",
       "      <td>1.192825e+01</td>\n",
       "      <td>2.425735e+01</td>\n",
       "      <td>1.543938e+00</td>\n",
       "      <td>1.555037e+00</td>\n",
       "      <td>4.398191e+00</td>\n",
       "      <td>3.441381e+00</td>\n",
       "      <td>5.954221e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID           Day          Hour          PRCP          SNOW  \\\n",
       "count  5.340900e+05  5.340900e+05  5.340900e+05  5.340900e+05  5.340900e+05   \n",
       "mean  -5.833968e-15 -2.426935e-14  1.807149e-14 -6.361288e-14 -1.527197e-14   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -1.796652e+01 -1.653279e+00 -1.993694e+00 -3.779970e-01 -1.358532e-01   \n",
       "25%   -3.557260e-01 -8.633741e-01 -6.294804e-01 -3.779970e-01 -1.358532e-01   \n",
       "50%    3.738473e-02  3.937406e-02  1.284163e-01 -3.779970e-01 -1.358532e-01   \n",
       "75%    4.263588e-01  8.292787e-01  8.863130e-01 -2.190015e-01 -1.358532e-01   \n",
       "max    3.472606e+00  1.732027e+00  1.492630e+00  1.192825e+01  2.425735e+01   \n",
       "\n",
       "               TMAX          TMIN  count_l_stops  count_bus_stops  \\\n",
       "count  5.340900e+05  5.340900e+05   5.340900e+05     5.340900e+05   \n",
       "mean  -1.510259e-14  1.499653e-13  -8.963044e-14    -1.005280e-15   \n",
       "std    1.000001e+00  1.000001e+00   1.000001e+00     1.000001e+00   \n",
       "min   -2.747795e+00 -2.873175e+00  -5.721857e-01    -2.123496e+00   \n",
       "25%   -7.970072e-01 -7.117863e-01  -5.721857e-01    -7.038846e-01   \n",
       "50%    1.783866e-01  2.624904e-02  -5.721857e-01    -3.063934e-01   \n",
       "75%    8.611624e-01  9.224348e-01   6.704085e-01     5.453734e-01   \n",
       "max    1.543938e+00  1.555037e+00   4.398191e+00     3.441381e+00   \n",
       "\n",
       "       count_metra_stops  ...      Beat_2521      Beat_2522      Beat_2523  \\\n",
       "count       5.340900e+05  ...  534090.000000  534090.000000  534090.000000   \n",
       "mean       -7.794098e-15  ...       0.004338       0.003073       0.002887   \n",
       "std         1.000001e+00  ...       0.065722       0.055345       0.053655   \n",
       "min        -4.499297e-01  ...       0.000000       0.000000       0.000000   \n",
       "25%        -4.499297e-01  ...       0.000000       0.000000       0.000000   \n",
       "50%        -4.499297e-01  ...       0.000000       0.000000       0.000000   \n",
       "75%        -4.499297e-01  ...       0.000000       0.000000       0.000000   \n",
       "max         5.954221e+00  ...       1.000000       1.000000       1.000000   \n",
       "\n",
       "           Beat_2524      Beat_2525      Beat_2531     Beat_2532  \\\n",
       "count  534090.000000  534090.000000  534090.000000  534090.00000   \n",
       "mean        0.002558       0.002273       0.003479       0.00454   \n",
       "std         0.050508       0.047622       0.058879       0.06723   \n",
       "min         0.000000       0.000000       0.000000       0.00000   \n",
       "25%         0.000000       0.000000       0.000000       0.00000   \n",
       "50%         0.000000       0.000000       0.000000       0.00000   \n",
       "75%         0.000000       0.000000       0.000000       0.00000   \n",
       "max         1.000000       1.000000       1.000000       1.00000   \n",
       "\n",
       "           Beat_2533      Beat_2534      Beat_2535  \n",
       "count  534090.000000  534090.000000  534090.000000  \n",
       "mean        0.005991       0.005198       0.003763  \n",
       "std         0.077173       0.071907       0.061231  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000  \n",
       "75%         0.000000       0.000000       0.000000  \n",
       "max         1.000000       1.000000       1.000000  \n",
       "\n",
       "[8 rows x 364 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.describe() #Looking fresh :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
