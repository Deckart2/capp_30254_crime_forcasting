{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpers to Run ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_mean(df_train, df_to_fill):\n",
    "    '''\n",
    "    A simple function that fills missing values of continuous columns \n",
    "        with the column median\n",
    "    Inputs:\n",
    "        df_train (df): the training df. Function computes means from this value AND fills this \n",
    "        df_to_fill (df): the df whose continuous NAs should be filled \n",
    "    Returns:\n",
    "        df_train (df): the training dataset\n",
    "        df_to_fill: the testing dataset with its data filled by the training data median\n",
    "\n",
    "    '''\n",
    "    df_train_num = df_train.select_dtypes(include=[np.number])\n",
    "    means = df_train_num.mean().to_dict()\n",
    "    \n",
    "    df_train = df_train.fillna(value=means)\n",
    "    df_to_fill = df_to_fill.fillna(value=means)\n",
    "    print(\"Finished filling NAs with mean...\")\n",
    "    return df_train, df_to_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_continuous(df, scaler = None):\n",
    "    '''\n",
    "    A simple function that normalizes the values of of continuous columns \n",
    "        using data from the training set\n",
    "    Inputs:\n",
    "        df (df): either the training or the testing df\n",
    "        scaler: the scaler object. It will be None for training and exist for testing \n",
    "    Returns:\n",
    "        df (df): the standardized df\n",
    "        scaler: the scaler object\n",
    "    '''\n",
    "    if scaler is None: #Training case\n",
    "        scaler = sk.preprocessing.StandardScaler() #Set up scaler\n",
    "        df_num = df.select_dtypes(include=[np.number]) #find numeric columns\n",
    "        df_num_scaled = scaler.fit_transform(df_num) #Normalize them\n",
    "        df_num_cols = list(df_num.columns) \n",
    "        df.loc[:, df_num_cols] = df_num_scaled #Insert columns back into the main df \n",
    "        print(\"Finished normalizing training data\")\n",
    "    else: #Testing case\n",
    "        df_num = df.select_dtypes(include=[np.number]) #find numeric columns\n",
    "        df_num_scaled = scaler.transform(df_num) #Normalize them\n",
    "        df_num_cols = list(df_num.columns) \n",
    "        df.loc[:, df_num_cols] = df_num_scaled #Insert columns back into the main df \n",
    "        print(\"Finished normalizing test data...\")\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, cat_vars): \n",
    "    '''\n",
    "    A function to one-hot encode given categorical variables\n",
    "    Inputs:\n",
    "        df (df): a pandas dataframe\n",
    "        cat_vars (list of strings): a list of the categorical variables to one-hot encode\n",
    "    '''\n",
    "\n",
    "    df = pd.get_dummies(df, columns = cat_vars)\n",
    "    print(\"finished one-hot encoding...\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(train, test):\n",
    "    '''\n",
    "    A function to ensure that training and testing data have identical columns\n",
    "    after one-hot encoding\n",
    "    If a column is in training but not testing, adds a column of 0s to testing\n",
    "    If column is in testing but not training, it is removed\n",
    "    Inputs:\n",
    "        train (df): the training df\n",
    "        test (df): the testing df\n",
    "    Outputs:\n",
    "        train, test (df): the datasets with identical columns\n",
    "    '''\n",
    "    train_cols = list(train.columns)\n",
    "    test_cols = list(test.columns)\n",
    "    \n",
    "    for tr_col in train_cols:\n",
    "        if tr_col not in test_cols:\n",
    "            test[tr_col] = 0\n",
    "    \n",
    "    for test_col in test_cols:\n",
    "        if test_col not in train_cols:\n",
    "            test = test.drop(test_col, axis=1)\n",
    "    print(\"finished standardizing...\")\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_year(df, y, test_year, num_years):\n",
    "    '''\n",
    "    isin syntax from: https://www.kite.com/python/answers/how-to-filter-a-pandas-dataframe-with-a-list-by-%60in%60-or-%60not-in%60-in-python\n",
    "    Inputs:\n",
    "        df (df): the dataframe with both the X and y \n",
    "        y (string): the column name in the df that is the target\n",
    "        test_year(int): the year we seek to predict in 2016-2020\n",
    "        num_years(int): the number of years to be included in the training set\n",
    "\n",
    "    Output:\n",
    "    train_df (df): A training dataframe\n",
    "    test_X (df): Testing df of predictors\n",
    "    test_y (df): Testing target\n",
    "    '''\n",
    "    year_range = np.arange(test_year - num_years, test_year)\n",
    "    train_filter = df.Year.isin(year_range)\n",
    "    train_df = df[train_filter]\n",
    "    train_X = train_df.drop(columns=[y])\n",
    "    train_y = train_df[y]\n",
    "    \n",
    "    test_df = df[df.Year==test_year]\n",
    "    test_X = test_df.drop(columns = [y])\n",
    "    test_y = test_df[y]\n",
    "    print(\"finished splitting by year...\")\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df, y, test_year, num_years, vars_to_onehot):\n",
    "    '''\n",
    "    Helper function that aggregates the above helpers to prepare for imputation in \n",
    "    an ML algorithm. Specifically this:\n",
    "        Splits the training set and testing set based on year \n",
    "            using split_train_test_by_year\n",
    "        One-hot encodes and standardizes the columns using \n",
    "            one_hot_encode and standardize_column\n",
    "        Normalizes all continuous variables using normalize_continuous\n",
    "    Inputs:\n",
    "    df (pandas DataFrame): the dataframe with training and testing data, predictors and target\n",
    "    y (string): the name of the target column\n",
    "    num_years (int): the number of years to be included in the training set\n",
    "    test_year (int): the year we seek to predict 2015_2020\n",
    "    \n",
    "    Outputs:\n",
    "    train_df (df): a standardized training set with one-hot encorded categorical columns\n",
    "    test_df (df): the test dataframe, again standardized as above\n",
    "    test_y (Series): the test target\n",
    "    '''\n",
    "    train_df, train_y, test_df, test_y = split_train_test_by_year(df, y, test_year, num_years)\n",
    "    train_df, test_df = standardize_columns(train_df, test_df)\n",
    "    train_df, test_df = fill_missing_mean(train_df, test_df)\n",
    "    train_df = one_hot_encode(train_df, vars_to_onehot)\n",
    "    #test_df = one_hot_encode(test_df, vars_to_onehot)\n",
    "    train_df, scaler = normalize_continuous(train_df)\n",
    "    test_df, doesnt_matter = normalize_continuous(test_df, scaler)\n",
    "    return train_df, train_y, test_df, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../intermediate_data/df_2015_to_present.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                       int64\n",
       "Arrest                    bool\n",
       "Domestic                  bool\n",
       "Beat                     int64\n",
       "Year                     int64\n",
       "Month                    int64\n",
       "Week                     int64\n",
       "Day                      int64\n",
       "Hour                     int64\n",
       "Watch                   object\n",
       "PRCP                   float64\n",
       "SNOW                   float64\n",
       "TMAX                     int64\n",
       "TMIN                     int64\n",
       "category_1              object\n",
       "category_2              object\n",
       "count_l_stops          float64\n",
       "count_bus_stops        float64\n",
       "count_metra_stops      float64\n",
       "count_restaurants      float64\n",
       "count_bars             float64\n",
       "count_daycares         float64\n",
       "count_entertainment    float64\n",
       "count_businesses       float64\n",
       "road_distance_ft       float64\n",
       "TOTAL POPULATION       float64\n",
       "dist_to_police         float64\n",
       "dist_to_hospital       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished splitting by year...\n"
     ]
    }
   ],
   "source": [
    "train_df, train_y, test_df, test_y = split_train_test_by_year(data, \"Arrest\", 2017, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished splitting by year...\n",
      "finished standardizing...\n",
      "Filling NAs with mean...\n",
      "finished one-hot encoding\n",
      "finished one-hot encoding\n",
      "Finished normalizing training data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 358 features, but this StandardScaler is expecting 360 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-eb256f895919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_X, train_y, test_X, test_y = prep_data(data, \"Arrest\", 2017, 2, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                        [\"Year\", \"Month\", \"Week\", \"Beat\"])\n",
      "\u001b[0;32m<ipython-input-71-ac53cf18ede7>\u001b[0m in \u001b[0;36mprep_data\u001b[0;34m(df, y, test_year, num_years, vars_to_onehot)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars_to_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_continuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoesnt_matter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_continuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-7f9368e7cc00>\u001b[0m in \u001b[0;36mnormalize_continuous\u001b[0;34m(df, scaler)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Testing case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdf_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#find numeric columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdf_num_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Normalize them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mdf_num_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_num_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_num_scaled\u001b[0m \u001b[0;31m#Insert columns back into the main df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         X = self._validate_data(X, reset=False,\n\u001b[0m\u001b[1;32m    792\u001b[0m                                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure_2d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 )\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0;34m'X has {} features, but this {} is expecting {} features '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     'as input.'.format(n_features, self.__class__.__name__,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 358 features, but this StandardScaler is expecting 360 features as input."
     ]
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = prep_data(data, \"Arrest\", 2017, 2, \n",
    "                                       [\"Year\", \"Month\", \"Week\", \"Beat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>count_l_stops</th>\n",
       "      <th>count_bus_stops</th>\n",
       "      <th>count_metra_stops</th>\n",
       "      <th>...</th>\n",
       "      <th>Beat_2521</th>\n",
       "      <th>Beat_2522</th>\n",
       "      <th>Beat_2523</th>\n",
       "      <th>Beat_2524</th>\n",
       "      <th>Beat_2525</th>\n",
       "      <th>Beat_2531</th>\n",
       "      <th>Beat_2532</th>\n",
       "      <th>Beat_2533</th>\n",
       "      <th>Beat_2534</th>\n",
       "      <th>Beat_2535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "      <td>5.340900e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.833968e-15</td>\n",
       "      <td>-2.426935e-14</td>\n",
       "      <td>1.807149e-14</td>\n",
       "      <td>-6.361288e-14</td>\n",
       "      <td>-1.527197e-14</td>\n",
       "      <td>-1.510259e-14</td>\n",
       "      <td>1.499653e-13</td>\n",
       "      <td>-8.963044e-14</td>\n",
       "      <td>-1.005280e-15</td>\n",
       "      <td>-7.794098e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>2.239702e-15</td>\n",
       "      <td>3.640538e-15</td>\n",
       "      <td>-2.780333e-15</td>\n",
       "      <td>-5.654102e-15</td>\n",
       "      <td>2.783987e-15</td>\n",
       "      <td>-4.568257e-14</td>\n",
       "      <td>-7.289202e-15</td>\n",
       "      <td>4.937708e-15</td>\n",
       "      <td>2.271241e-14</td>\n",
       "      <td>-3.035760e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.796652e+01</td>\n",
       "      <td>-1.653279e+00</td>\n",
       "      <td>-1.993694e+00</td>\n",
       "      <td>-3.779970e-01</td>\n",
       "      <td>-1.358532e-01</td>\n",
       "      <td>-2.747795e+00</td>\n",
       "      <td>-2.873175e+00</td>\n",
       "      <td>-5.721857e-01</td>\n",
       "      <td>-2.123496e+00</td>\n",
       "      <td>-4.499297e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.600850e-02</td>\n",
       "      <td>-5.551563e-02</td>\n",
       "      <td>-5.380998e-02</td>\n",
       "      <td>-5.063773e-02</td>\n",
       "      <td>-4.773053e-02</td>\n",
       "      <td>-5.908434e-02</td>\n",
       "      <td>-6.753623e-02</td>\n",
       "      <td>-7.763771e-02</td>\n",
       "      <td>-7.228266e-02</td>\n",
       "      <td>-6.146241e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.557260e-01</td>\n",
       "      <td>-8.633741e-01</td>\n",
       "      <td>-6.294804e-01</td>\n",
       "      <td>-3.779970e-01</td>\n",
       "      <td>-1.358532e-01</td>\n",
       "      <td>-7.970072e-01</td>\n",
       "      <td>-7.117863e-01</td>\n",
       "      <td>-5.721857e-01</td>\n",
       "      <td>-7.038846e-01</td>\n",
       "      <td>-4.499297e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.600850e-02</td>\n",
       "      <td>-5.551563e-02</td>\n",
       "      <td>-5.380998e-02</td>\n",
       "      <td>-5.063773e-02</td>\n",
       "      <td>-4.773053e-02</td>\n",
       "      <td>-5.908434e-02</td>\n",
       "      <td>-6.753623e-02</td>\n",
       "      <td>-7.763771e-02</td>\n",
       "      <td>-7.228266e-02</td>\n",
       "      <td>-6.146241e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.738473e-02</td>\n",
       "      <td>3.937406e-02</td>\n",
       "      <td>1.284163e-01</td>\n",
       "      <td>-3.779970e-01</td>\n",
       "      <td>-1.358532e-01</td>\n",
       "      <td>1.783866e-01</td>\n",
       "      <td>2.624904e-02</td>\n",
       "      <td>-5.721857e-01</td>\n",
       "      <td>-3.063934e-01</td>\n",
       "      <td>-4.499297e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.600850e-02</td>\n",
       "      <td>-5.551563e-02</td>\n",
       "      <td>-5.380998e-02</td>\n",
       "      <td>-5.063773e-02</td>\n",
       "      <td>-4.773053e-02</td>\n",
       "      <td>-5.908434e-02</td>\n",
       "      <td>-6.753623e-02</td>\n",
       "      <td>-7.763771e-02</td>\n",
       "      <td>-7.228266e-02</td>\n",
       "      <td>-6.146241e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.263588e-01</td>\n",
       "      <td>8.292787e-01</td>\n",
       "      <td>8.863130e-01</td>\n",
       "      <td>-2.190015e-01</td>\n",
       "      <td>-1.358532e-01</td>\n",
       "      <td>8.611624e-01</td>\n",
       "      <td>9.224348e-01</td>\n",
       "      <td>6.704085e-01</td>\n",
       "      <td>5.453734e-01</td>\n",
       "      <td>-4.499297e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.600850e-02</td>\n",
       "      <td>-5.551563e-02</td>\n",
       "      <td>-5.380998e-02</td>\n",
       "      <td>-5.063773e-02</td>\n",
       "      <td>-4.773053e-02</td>\n",
       "      <td>-5.908434e-02</td>\n",
       "      <td>-6.753623e-02</td>\n",
       "      <td>-7.763771e-02</td>\n",
       "      <td>-7.228266e-02</td>\n",
       "      <td>-6.146241e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.472606e+00</td>\n",
       "      <td>1.732027e+00</td>\n",
       "      <td>1.492630e+00</td>\n",
       "      <td>1.192825e+01</td>\n",
       "      <td>2.425735e+01</td>\n",
       "      <td>1.543938e+00</td>\n",
       "      <td>1.555037e+00</td>\n",
       "      <td>4.398191e+00</td>\n",
       "      <td>3.441381e+00</td>\n",
       "      <td>5.954221e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.514956e+01</td>\n",
       "      <td>1.801294e+01</td>\n",
       "      <td>1.858391e+01</td>\n",
       "      <td>1.974812e+01</td>\n",
       "      <td>2.095095e+01</td>\n",
       "      <td>1.692496e+01</td>\n",
       "      <td>1.480687e+01</td>\n",
       "      <td>1.288034e+01</td>\n",
       "      <td>1.383458e+01</td>\n",
       "      <td>1.627011e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 360 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID           Day          Hour          PRCP          SNOW  \\\n",
       "count  5.340900e+05  5.340900e+05  5.340900e+05  5.340900e+05  5.340900e+05   \n",
       "mean  -5.833968e-15 -2.426935e-14  1.807149e-14 -6.361288e-14 -1.527197e-14   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -1.796652e+01 -1.653279e+00 -1.993694e+00 -3.779970e-01 -1.358532e-01   \n",
       "25%   -3.557260e-01 -8.633741e-01 -6.294804e-01 -3.779970e-01 -1.358532e-01   \n",
       "50%    3.738473e-02  3.937406e-02  1.284163e-01 -3.779970e-01 -1.358532e-01   \n",
       "75%    4.263588e-01  8.292787e-01  8.863130e-01 -2.190015e-01 -1.358532e-01   \n",
       "max    3.472606e+00  1.732027e+00  1.492630e+00  1.192825e+01  2.425735e+01   \n",
       "\n",
       "               TMAX          TMIN  count_l_stops  count_bus_stops  \\\n",
       "count  5.340900e+05  5.340900e+05   5.340900e+05     5.340900e+05   \n",
       "mean  -1.510259e-14  1.499653e-13  -8.963044e-14    -1.005280e-15   \n",
       "std    1.000001e+00  1.000001e+00   1.000001e+00     1.000001e+00   \n",
       "min   -2.747795e+00 -2.873175e+00  -5.721857e-01    -2.123496e+00   \n",
       "25%   -7.970072e-01 -7.117863e-01  -5.721857e-01    -7.038846e-01   \n",
       "50%    1.783866e-01  2.624904e-02  -5.721857e-01    -3.063934e-01   \n",
       "75%    8.611624e-01  9.224348e-01   6.704085e-01     5.453734e-01   \n",
       "max    1.543938e+00  1.555037e+00   4.398191e+00     3.441381e+00   \n",
       "\n",
       "       count_metra_stops  ...     Beat_2521     Beat_2522     Beat_2523  \\\n",
       "count       5.340900e+05  ...  5.340900e+05  5.340900e+05  5.340900e+05   \n",
       "mean       -7.794098e-15  ...  2.239702e-15  3.640538e-15 -2.780333e-15   \n",
       "std         1.000001e+00  ...  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min        -4.499297e-01  ... -6.600850e-02 -5.551563e-02 -5.380998e-02   \n",
       "25%        -4.499297e-01  ... -6.600850e-02 -5.551563e-02 -5.380998e-02   \n",
       "50%        -4.499297e-01  ... -6.600850e-02 -5.551563e-02 -5.380998e-02   \n",
       "75%        -4.499297e-01  ... -6.600850e-02 -5.551563e-02 -5.380998e-02   \n",
       "max         5.954221e+00  ...  1.514956e+01  1.801294e+01  1.858391e+01   \n",
       "\n",
       "          Beat_2524     Beat_2525     Beat_2531     Beat_2532     Beat_2533  \\\n",
       "count  5.340900e+05  5.340900e+05  5.340900e+05  5.340900e+05  5.340900e+05   \n",
       "mean  -5.654102e-15  2.783987e-15 -4.568257e-14 -7.289202e-15  4.937708e-15   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -5.063773e-02 -4.773053e-02 -5.908434e-02 -6.753623e-02 -7.763771e-02   \n",
       "25%   -5.063773e-02 -4.773053e-02 -5.908434e-02 -6.753623e-02 -7.763771e-02   \n",
       "50%   -5.063773e-02 -4.773053e-02 -5.908434e-02 -6.753623e-02 -7.763771e-02   \n",
       "75%   -5.063773e-02 -4.773053e-02 -5.908434e-02 -6.753623e-02 -7.763771e-02   \n",
       "max    1.974812e+01  2.095095e+01  1.692496e+01  1.480687e+01  1.288034e+01   \n",
       "\n",
       "          Beat_2534     Beat_2535  \n",
       "count  5.340900e+05  5.340900e+05  \n",
       "mean   2.271241e-14 -3.035760e-15  \n",
       "std    1.000001e+00  1.000001e+00  \n",
       "min   -7.228266e-02 -6.146241e-02  \n",
       "25%   -7.228266e-02 -6.146241e-02  \n",
       "50%   -7.228266e-02 -6.146241e-02  \n",
       "75%   -7.228266e-02 -6.146241e-02  \n",
       "max    1.383458e+01  1.627011e+01  \n",
       "\n",
       "[8 rows x 360 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
