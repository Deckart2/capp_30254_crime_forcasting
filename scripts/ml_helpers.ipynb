{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpers to Run ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_mean(df_train, df_to_fill):\n",
    "    '''\n",
    "    A simple function that fills missing values of continuous columns \n",
    "        with the column median\n",
    "    Inputs:\n",
    "        df_train (df): the training df. Function computes means from this value AND fills this \n",
    "        df_to_fill (df): the df whose continuous NAs should be filled \n",
    "    Returns:\n",
    "        df_train (df): the training dataset\n",
    "        df_to_fill: the testing dataset with its data filled by the training data median\n",
    "\n",
    "    '''\n",
    "    df_train_num = df_train.select_dtypes(include=[np.number])\n",
    "    #means = df_train_num.mean().to_dict()\n",
    "    mean_dict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for col in df_train_num.columns:\n",
    "        mean_dict[col] = df_train[col].mean()\n",
    "    \n",
    "    df_train.fillna(value=mean_dict, inplace=True)\n",
    "    df_to_fill.fillna(value=mean_dict, inplace=True)\n",
    "    \n",
    "    #df_train = df_train.fillna(value=means)\n",
    "    #df_to_fill = df_to_fill.fillna(value=means)\n",
    "    print(\"Finished filling NAs with mean...\")\n",
    "    return df_train, df_to_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing above function\n",
    "\n",
    "#df_train = pd.DataFrame({'value': [1, np.nan, np.nan, 2, 3, 1, 3, np.nan, 3], \"value2\":[1, 2, 3, 4, 5, np.nan, 7, 8, 9], 'Year': ['A','A', 'B','B','B','B', 'C','C','C']})\n",
    "#df_to_fill = pd.DataFrame({'value': [1, np.nan, np.nan, 2, 3, 1, 3, np.nan, 3], \"value2\":[10, 20, 33, 43, 53, np.nan, 7, 8, 9], 'Year': ['A','A', 'B','B','B','B', 'C','C','C']})\n",
    "#df_train, df_to_fill = fill_missing_mean(df_train, df_to_fill)\n",
    "#df_to_fill\n",
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_continuous(df, scaler = None):\n",
    "    '''\n",
    "    A simple function that normalizes the values of of continuous columns \n",
    "        using data from the training set\n",
    "    Inputs:\n",
    "        df (df): either the training or the testing df\n",
    "        scaler: the scaler object. It will be None for training and exist for testing \n",
    "    Returns:\n",
    "        df (df): the standardized df\n",
    "        scaler: the scaler object\n",
    "    '''\n",
    "    df[\"Year\"]=df[\"Year\"].astype(\"category\")\n",
    "    if scaler is None: #Training case\n",
    "        scaler = sk.preprocessing.StandardScaler() #Set up scaler\n",
    "        df_num = df.select_dtypes(include=[np.number]) #find numeric columns\n",
    "        df_num_scaled = scaler.fit_transform(df_num) #Normalize them\n",
    "        df_num_cols = list(df_num.columns) \n",
    "        df.loc[:, df_num_cols] = df_num_scaled #Insert columns back into the main df \n",
    "        print(\"Finished normalizing training data\")\n",
    "    else: #Testing case\n",
    "        df_num = df.select_dtypes(include=[np.number]) #find numeric columns\n",
    "        df_num_scaled = scaler.transform(df_num) #Normalize them\n",
    "        df_num_cols = list(df_num.columns) \n",
    "        df.loc[:, df_num_cols] = df_num_scaled #Insert columns back into the main df \n",
    "        print(\"Finished normalizing test data...\")\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, cat_vars): \n",
    "    '''\n",
    "    A function to one-hot encode given categorical variables\n",
    "    Inputs:\n",
    "        df (df): a pandas dataframe\n",
    "        cat_vars (list of strings): a list of the categorical variables to one-hot encode\n",
    "    '''\n",
    "\n",
    "    df = pd.get_dummies(df, columns = cat_vars)\n",
    "    print(\"Finished one-hot encoding...\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(train, test):\n",
    "    '''\n",
    "    A function to ensure that training and testing data have identical columns\n",
    "    after one-hot encoding\n",
    "    If a column is in training but not testing, adds a column of 0s to testing\n",
    "    If column is in testing but not training, it is removed\n",
    "    Inputs:\n",
    "        train (df): the training df\n",
    "        test (df): the testing df\n",
    "    Outputs:\n",
    "        train, test (df): the datasets with identical columns\n",
    "    '''\n",
    "    train_cols = list(train.columns)\n",
    "    test_cols = list(test.columns)\n",
    "    \n",
    "    for tr_col in train_cols:\n",
    "        if tr_col not in test_cols:\n",
    "            test[tr_col] = 0\n",
    "    \n",
    "    for test_col in test_cols:\n",
    "        if test_col not in train_cols:\n",
    "            test = test.drop(test_col, axis=1)\n",
    "    print(\"Finished standardizing...\")\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_year(df, y, num_years, year_col):\n",
    "    '''\n",
    "    \n",
    "    Ultimately, this function divides the dataset up into smaller chunks with a one year test set and a num_years \n",
    "        years worth of training data in the num_years years just previous to the test year\n",
    "        \n",
    "    Ex: we are always predicting 2020 but remove that to see results later.\n",
    "        If num_years = 2 and the data runs from 2015 to 2020, this function creates data with:\n",
    "            \n",
    "            set 1:\n",
    "                Train: 2015 and 2016\n",
    "                Test: 2017\n",
    "            set 2:\n",
    "                Train: 2016 2017\n",
    "                Test: 2018\n",
    "            set 3:\n",
    "                Train: 2017 and 2018\n",
    "                Test: 2019\n",
    "    Inputs:\n",
    "        df (df): the dataframe with both the X and y \n",
    "        y (string): the column name in the df that is the target\n",
    "        num_years(int): the number of years to be included in the training set\n",
    "        year_col (str): the name of the column representing the years in the df\n",
    "    Output:\n",
    "   train_test_data_list (list of tuples):\n",
    "       each tuple contains:\n",
    "           df_train (DataFrame): includes num_years worth of data before the test year\n",
    "           df_test (DataFrame): includes 1 year, the test year for this set of data\n",
    "    '''\n",
    "    year_list = df[year_col].unique()\n",
    "    year_list = sorted(list(year_list), reverse=True)\n",
    "    year_list\n",
    "\n",
    "    train_test_data_list = []\n",
    "    for year in year_list:\n",
    "        if year - num_years in year_list:\n",
    "            df_test = df.loc[df[year_col]==year]\n",
    "            df_train = df.loc[(df[year_col] < year) & (df[year_col] >= year-num_years)]\n",
    "            train_test_data_list.append((df_train, df_test, year))\n",
    "            print(train_test_data_list[-1][0])\n",
    "    train_test_data_list.pop(0)\n",
    "    print(\"Finished splitting...\")\n",
    "    return train_test_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_year_test(df, y, num_years, year_col):\n",
    "    '''\n",
    "    \n",
    "    Ultimately, this function divides the dataset up into smaller chunks with a one year test set and a num_years \n",
    "        years worth of training data in the num_years years just previous to the test year\n",
    "        \n",
    "    Ex: we are always predicting 2020 but remove that to see results later.\n",
    "        If num_years = 2 and the data runs from 2015 to 2020, this function creates data with:\n",
    "            \n",
    "            set 1:\n",
    "                Train: 2015 and 2016\n",
    "                Test: 2017\n",
    "            set 2:\n",
    "                Train: 2016 2017\n",
    "                Test: 2018\n",
    "            set 3:\n",
    "                Train: 2017 and 2018\n",
    "                Test: 2019\n",
    "    Inputs:\n",
    "        df (df): the dataframe with both the X and y \n",
    "        y (string): the column name in the df that is the target\n",
    "        num_years(int): the number of years to be included in the training set\n",
    "        year_col (str): the name of the column representing the years in the df\n",
    "    Output:\n",
    "   train_test_data_list (list of tuples):\n",
    "       each tuple contains:\n",
    "           df_train (DataFrame): includes num_years worth of data before the test year\n",
    "           df_test (DataFrame): includes 1 year, the test year for this set of data\n",
    "    '''\n",
    "    year_list = df[year_col].unique()\n",
    "    year_list = sorted(list(year_list), reverse=True)\n",
    "    year_list\n",
    "\n",
    "    train_test_data_list = []\n",
    "    for year in year_list:\n",
    "        if year - num_years in year_list:\n",
    "            df_test = df.loc[df[year_col]==year]\n",
    "            df_train = df.loc[(df[year_col] < year) & (df[year_col] >= year-num_years)]\n",
    "            train_test_data_list.append((df_train, df_test, year))\n",
    "            print(train_test_data_list[-1][0])\n",
    "    print(\"Finished splitting...\")\n",
    "    return train_test_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing above;\n",
    "#data_list = split_train_test_by_year(data, \"was_arrested\", 2, \"Year\")   \n",
    "#for group in data_list:\n",
    "#    print(group[0][\"Year\"].unique(), group[1][\"Year\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df, y, num_years, year_col, vars_to_onehot):\n",
    "    '''\n",
    "    Helper function that aggregates the above helpers to prepare for imputation in \n",
    "    an ML algorithm. Specifically this:\n",
    "        Splits the training set and testing set based on year \n",
    "            using split_train_test_by_year\n",
    "        One-hot encodes and standardizes the columns using \n",
    "            one_hot_encode and standardize_column\n",
    "        Normalizes all continuous variables using normalize_continuous\n",
    "    Inputs:\n",
    "    df (pandas DataFrame): the dataframe with training and testing data, predictors and target\n",
    "    y (string): the name of the target column\n",
    "    num_years (int): the number of years to be included in the training set\n",
    "    test_year (int): the year we seek to predict 2015_2020\n",
    "    \n",
    "    Outputs:\n",
    "    cleaned_train_test - a list of tuples. \n",
    "    The first tuple is the training dataframe and the second is the test for a given set of years\n",
    "    '''\n",
    "    df = convert_to_categorical(df, [y])\n",
    "    cleaned_trained_test = []\n",
    "    train_test_list = split_train_test_by_year_test(df, y, num_years, year_col)\n",
    "    for year_set in train_test_list:\n",
    "        print(\"Working on:\", year_set[0][\"Year\"].unique())\n",
    "        train_df = year_set[0]\n",
    "        test_df = year_set[1]\n",
    "        year = year_set[2]\n",
    "        print(\"Have accessed train and test df...\")\n",
    "        train_df, test_df = fill_missing_mean(train_df, test_df)\n",
    "        print(\"On to normalizing continuous...\")\n",
    "        train_df, scaler = normalize_continuous(train_df)\n",
    "        test_df, doesnt_matter = normalize_continuous(test_df, scaler)\n",
    "        train_df = one_hot_encode(train_df, vars_to_onehot)\n",
    "        test_df = one_hot_encode(test_df, vars_to_onehot)\n",
    "        train_df, test_df = standardize_columns(train_df, test_df)\n",
    "        \n",
    "        cleaned_trained_test.append((train_df, test_df, year))\n",
    "        \n",
    "    return cleaned_trained_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_test(df, y, num_years, year_col, vars_to_onehot):\n",
    "    '''\n",
    "    This function is identical to the above except that it does not remove the last set of data\n",
    "    Thus, it should be used when computing final results of the best models\n",
    "    '''\n",
    "    df = convert_to_categorical(df, [y])\n",
    "    cleaned_trained_test = []\n",
    "    train_test_list = split_train_test_by_year_test(df, y, num_years, year_col)\n",
    "    for year_set in train_test_list:\n",
    "        print(\"Working on:\", year_set[0][\"Year\"].unique())\n",
    "        train_df = year_set[0]\n",
    "        test_df = year_set[1]\n",
    "        year = year_set[2]\n",
    "        print(\"Have accessed train and test df...\")\n",
    "        train_df, test_df = fill_missing_mean(train_df, test_df)\n",
    "        print(\"On to normalizing continuous...\")\n",
    "        train_df, scaler = normalize_continuous(train_df)\n",
    "        test_df, doesnt_matter = normalize_continuous(test_df, scaler)\n",
    "        train_df = one_hot_encode(train_df, vars_to_onehot)\n",
    "        test_df = one_hot_encode(test_df, vars_to_onehot)\n",
    "        train_df, test_df = standardize_columns(train_df, test_df)\n",
    "        \n",
    "        cleaned_trained_test.append((train_df, test_df, year))\n",
    "        \n",
    "    return cleaned_trained_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_categorical(df, cols_to_convert):\n",
    "    '''\n",
    "    Convert columns to categorical\n",
    "    Inputs:\n",
    "        df (pd.DataFrame): The Pandas df\n",
    "        cols_to_convert (list of strings): The columns to convert    \n",
    "    Output:\n",
    "        df - the updated dataframe\n",
    "    '''\n",
    "    for col in cols_to_convert:\n",
    "        df[col]=df[col].astype(\"category\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up test data:\n",
    "#data = pd.read_csv(\"../intermediate_data/df_2015_to_present.csv\")\n",
    "#data.head(5)\n",
    "#data[\"was_arrested\"]=data[\"Arrest\"].astype(\"float\")\n",
    "#data = data.drop(\"Arrest\", axis = 1)\n",
    "#data = convert_to_categorical(data, [\"Beat\", \"Month\", \"Watch\"])\n",
    "#data.head(5)\n",
    "\n",
    "#data_small = data.sample(frac=0.0001)\n",
    "#data_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_small.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code! \n",
    "#data_list  = prep_data_test(data_small, \"was_arrested\", 2, \"Year\", \n",
    "#                                       [\"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X_y(data, y):\n",
    "    '''\n",
    "    Function that separates predictors from target\n",
    "    Inputs:\n",
    "        data (DF): dataframe\n",
    "        y (string): the name of the data column\n",
    "    Outputs:\n",
    "        X_ - df with predictor data\n",
    "        y_ - df with target data\n",
    "    '''\n",
    "    y_ = data[y]\n",
    "    X_ = data.drop(y, axis=1)\n",
    "    \n",
    "    return X_, y_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train = split_x_y(data, \"Arrest\")\n",
    "#y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
